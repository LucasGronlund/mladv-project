{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fetch_data as fd\n",
    "import svm\n",
    "from random import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import kernel\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import time\n",
    "import subprocess\n",
    "import io\n",
    "import sys\n",
    "import alignment\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseMatrix(matrix):\n",
    "    num_rows = int(matrix[0])\n",
    "    num_cols = int(matrix[1])\n",
    "    K = np.zeros([num_rows,num_cols])\n",
    "    counter = 2\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            K[i,j] = float(matrix[counter])\n",
    "            counter = counter + 1\n",
    "    return K\n",
    "\n",
    "def marcus_recursive_kernel(s,t,n,l):\n",
    "    the_strings = b''\n",
    "    the_args = bytes(str(len(s)),'ascii')+ b' ' + bytes(str(len(t)),'ascii')+ b' '\n",
    "    for i in s:\n",
    "        the_strings = the_strings + bytes(i,'ascii')\n",
    "        the_args = the_args + bytes(str(len(i)),'ascii') + b' '\n",
    "    for i in t:\n",
    "        the_strings = the_strings + bytes(i,'ascii')\n",
    "        the_args = the_args + bytes(str(len(i)),'ascii') + b' '\n",
    "    the_args = the_args + bytes(str(n), 'ascii')+ b' ' + bytes(str(l), 'ascii')\n",
    "    fast_kernel = subprocess.Popen([\"./fast_recursive_kernel.out\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    output_test = fast_kernel.communicate(input=b'1 '+the_args+the_strings)[0]\n",
    "    output_test_list = output_test.decode('utf-8').split()\n",
    "    K = parseMatrix(output_test_list)\n",
    "    return K\n",
    "\n",
    "def create_ngrams(text, n):\n",
    "    \"\"\"Create a set of ngrams of length n\"\"\"\n",
    "    return set(text[i:i+n] for i in range(len(text)-n+1))\n",
    "\n",
    "def wk(doc1, doc2):\n",
    "    #print \"Creating the bag of words...\\n\"\n",
    "    clean_docs = [doc1,doc2]\n",
    "    #defaultanalyzer \"word\" removes non-chars in preprocessing and tokenizes words. does not remove \"markup tokens\"\n",
    "    #stop_words should be \"english\" if not using clean_input_docs()\n",
    "    #vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "    #                             tokenizer = None,    \\\n",
    "    #                             preprocessor = None, \\\n",
    "    #                             stop_words = \"english\") \n",
    "    vectorizer = CountVecotizer(analyzer)\n",
    "\n",
    "    train_data_features = vectorizer.fit_transform(clean_docs)\n",
    "    train_data_features = train_data_features.toarray()\n",
    "\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    tfidf = transformer.fit_transform(train_data_features)\n",
    "    tfidf = tfidf.toarray() \n",
    "    return np.dot(tfidf[0],tfidf[1])\n",
    "\n",
    "\n",
    "def ngk(doc1, doc2, n,luc=False):\n",
    "    if(luc):\n",
    "        p1 = [doc1[i:i+n] for i in range(len(doc1)-n+1)]\n",
    "        p2 = [doc2[i:i+n] for i in range(len(doc2)-n+1)]\n",
    "        same = 0;\n",
    "        unique = 0;\n",
    "        same = len(np.intersect1d(p1,p2))\n",
    "        unique = same+len(np.setdiff1d(p1,p2))+len(np.setdiff1d(p2,p1));\n",
    "        return same/unique\n",
    "    else:\n",
    "        sd1 = create_ngrams(doc1, n)\n",
    "        sd2 = create_ngrams(doc2, n)\n",
    "\n",
    "        if len(sd1 | sd2) == 0:\n",
    "            return 1.0\n",
    "\n",
    "        return len(sd1 & sd2) * 1.0 / len(sd1 | sd2)\n",
    "\n",
    "    \n",
    "\n",
    "def testNGK(train,trainLabels,test,testLabels,n=2,luc=False):\n",
    "    \n",
    "    clf = svm.SVC(kernel='precomputed')\n",
    "    \n",
    "    Ktrain = np.zeros([len(train),len(train)]);\n",
    "    #computeing train\n",
    "    print(\"beginning to train\")\n",
    "    for i in tqdm(range(len(train))):\n",
    "        for j in range(len(train)):\n",
    "            Ktrain[i][j] = ngk(train[i],train[j],n,luc);\n",
    "    \n",
    "    Ktest = np.zeros([len(train),len(test)]);\n",
    "    print(\"done training. \\n starting to predict\")\n",
    "    for i in tqdm(range(len(train))):\n",
    "        for j in range(len(test)):\n",
    "            Ktest[i][j] = ngk(train[i],test[j],n,luc)\n",
    "            \n",
    "    clf.fit(Ktrain,trainLabels)\n",
    "    \n",
    "    pred = clf.predict(Ktest.T)\n",
    "    \n",
    "    prec, rec, fsc, supp = metrics.precision_recall_fscore_support(testLabels, pred)\n",
    "    \n",
    "    return prec, rec, fsc, supp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "#generate data\n",
    "\n",
    "categories = ['earn','acq','crude','corn']\n",
    "numberOfTraining = [152,114,76,38]\n",
    "numberOfTesting = [40,25,15,10]\n",
    "\n",
    "trainData,trainLabels, testData,testLabel = fd.loadData(categories,numberOfTraining,numberOfTesting)\n",
    "\n",
    "minTrain = 100;\n",
    "for i in trainData:\n",
    "    if(len(i)<minTrain):\n",
    "        minTrain = len(i)\n",
    "        \n",
    "minTest = 100;\n",
    "for i in testData:\n",
    "    if(len(i)<minTest):\n",
    "        minTest = len(i)\n",
    "        \n",
    "print(minTrain)\n",
    "print(minTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all data \n",
    "\n",
    "cwd = os.getcwd()+'\\..\\data\\clean_data';\n",
    "kerns = os.path.join(cwd,'trainData_8.npy')\n",
    "np.save(kerns, trainData) \n",
    "\n",
    "cwd = os.getcwd()+'\\..\\data\\clean_data';\n",
    "kerns = os.path.join(cwd,'testData_8.npy')\n",
    "np.save(kerns, testData) \n",
    "\n",
    "cwd = os.getcwd()+'\\..\\data\\clean_data';\n",
    "kerns = os.path.join(cwd,'trainLabels_8.npy')\n",
    "np.save(kerns, trainLabels) \n",
    "\n",
    "cwd = os.getcwd()+'\\..\\data\\clean_data';\n",
    "kerns = os.path.join(cwd,'testLabels_8.npy')\n",
    "np.save(kerns, testLabel) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "cwd = os.getcwd()+'\\..\\data\\clean_data';\n",
    "kerns = os.path.join(cwd,'trainData.npy')\n",
    "trainData = np.load(kerns)\n",
    "\n",
    "kerns = os.path.join(cwd,'testData.npy')\n",
    "testData = np.load(kerns)\n",
    "\n",
    "kerns = os.path.join(cwd,'trainLabels.npy')\n",
    "trainLabels = np.load(kerns)\n",
    "\n",
    "kerns = os.path.join(cwd,'testLabels.npy')\n",
    "testLabels = np.load(kerns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marcus: 1331.7753159999847\n"
     ]
    }
   ],
   "source": [
    "n=4\n",
    "l=0.5\n",
    "\n",
    "t1=time.time()\n",
    "K = marcus_recursive_kernel(trainData,trainData,n,l)\n",
    "t2=time.time()\n",
    "\n",
    "print(\"marcus: \"+repr(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()+'\\..\\data\\precomp_recursive_kernels';\n",
    "kerns = os.path.join(cwd,'n4l05Train.npy')\n",
    "np.save(kerns,K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()+'\\..\\data\\precomp_recursive_kernels';\n",
    "kerns = os.path.join(cwd,'n3l05Train.npy')\n",
    "Ktrain = np.load(kerns)\n",
    "\n",
    "kerns = os.path.join(cwd,'n3l05Test.npy')\n",
    "Ktest = np.load(kerns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 3 3 3 2 3 3 2 3]\n"
     ]
    }
   ],
   "source": [
    "earnTrain = np.array([0]*152)\n",
    "acqTrain = np.array([1]*114)\n",
    "crudeTrain = [2]*76\n",
    "cornTrain = [3]*38\n",
    "trainLbl = np.concatenate([earnTrain,acqTrain,crudeTrain,cornTrain])\n",
    "\n",
    "earnTest = np.array([0]*40)\n",
    "acqTest = np.array([1]*25)\n",
    "crudeTest = [2]*15\n",
    "cornTest = [3]*10\n",
    "testLbl = np.concatenate([earnTest,acqTest,crudeTest,cornTest])\n",
    "\n",
    "clf = svm.SVC(kernel='precomputed');\n",
    "\n",
    "clf.fit(Ktrain,trainLbl);\n",
    "\n",
    "pred = clf.predict(Ktest)\n",
    "\n",
    "print(pred)\n",
    "\n",
    "prec, rec, fsc, supp = metrics.precision_recall_fscore_support(testLbl, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97297297  0.82758621  0.77777778  1.        ]\n",
      "[ 0.9         0.96        0.93333333  0.6       ]\n",
      "[ 0.93506494  0.88888889  0.84848485  0.75      ]\n"
     ]
    }
   ],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(fsc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
