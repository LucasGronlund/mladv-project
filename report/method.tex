% TODO
\subsection{String subsequence kernel}
The method Lodhi et al. proposes is to compare documents by the means of the substrings they contain, the more substrings two documents have in common the more similar they are. These substrings do however not need to be contiguous, which is what sets this this method of comparison apart from the word kernel's and n-gram kernel's method.

The feature vectors for each document is thus created through first generating all non-contiguous substrings of length k from all the documents. Each such substring is then regarded as a dimension of the feature space. The transformation of the document to this feature space is then performed "simply" trough finding all occurrences of these substrings and weighing them according to how compactly the substring is embedded in the text. This is handled through the introduction of a decay factor $ \lambda \in (0,1) $, which lets us put smaller emphasis for less compact substrings than ones more compact. An example of this, presented by Lodhi et al., is if we consider the substring (dimension of the feature space) \textbf{c-a-r}. This is more compactly represented in the document '\textbf{car}d', than in the document '\textbf{c}ust\textbf{ar}d' giving the former a value of $ \lambda^3 $ and the latter $ \lambda^6 $ (where the exponent directly corresponds to the length of the sequence for which the feature is contained. )

To not introduce unnecessary confusion we will present the definition of a SSK exactly like the definition presented in Lohdi et al. 

\subsubsection*{Definition 1 - String subsequence kernel}
Let $ \Sigma $ be a finite alphabet. A string is a finite seqence of characters from $ \Sigma $, including the empty sequence. For strings $s,t$ we denote by $|s|$ the length of the string $ s = s_1 \dots s_{|s|} $, and by $ st $ the string obtained by concatenating the strings $ s $ and $ t $. The string $ s[i:j] $ is the substring $ s_i, \dots s_j $ of $ s $. We say that $ u $ us a subsequence of $ s $, if there exist indices $ \boldsymbol{i} = (i_1, \dots, i_{|u|}) $, with $ 1 \leq i_1 < \dots < i_{|u|} \leq |s| $, such that $ u_j = s_{i_{j}}$, for $ j = 1, \dots, |u|, $ or $ u = s[\boldsymbol{i}] $ for short. The length $ l(\boldsymbol{i}) $ of the subsequence in $ s $ is $ i_{|u|} - i_1 +1 $. We denote by $ \Sigma^n $ the set of all finite strings of length n, and by $ \Sigma^* $ the set of all strings 
\begin{equation}\label{eq:all_Strings}
\Sigma^* = \bigcup^{\infty}_{n=0}\Sigma^n
\end{equation}
We now define the feature spaces $ F_n = \mathbb{R}^{\Sigma^{n}} $. The feature mapping $ \phi $ for a string $ s $ is given by defining the $ u $ coordinate $ \phi_u(s) $ for each $u \in \Sigma^n  $ We define 
\begin{equation}
\phi_u(s) = \sum_{\boldsymbol{i}:u=s[\boldsymbol{i}]} \lambda^{l(\boldsymbol{i})}
\end{equation}
for some $ \lambda \in (0,1) $. These features measure the number of occurrences of subsequences in the string $ s $ weighting them according to their lenghts. Hence, the inner product of the feature vectors for two strings $ s $ and $ t $ give a sum over all common subsequences weighted according to their frequency of occurrence and lengths
\begin{align*}\label{key}
K_n(s,t) &= \sum_{u\in\Sigma^n} \langle \phi_u(s) \cdot \phi_u(t) \rangle = \sum_{u \in \Sigma^n} \sum_{\boldsymbol{i}:u=s[\boldsymbol{i}]} \lambda^{l(\boldsymbol{i})} \sum_{\boldsymbol{j}:u=t[\boldsymbol{j}]} \lambda^{l(\boldsymbol{j})} \\
& =  \sum_{u \in \Sigma^n} \sum_{\boldsymbol{i}:u=s[\boldsymbol{i}]}  \sum_{\boldsymbol{j}:u=t[\boldsymbol{j}]} \lambda^{l(\boldsymbol{i}) + l(\boldsymbol{j})}
\end{align*}


To prove our understanding of this definition we construct a unique example, which also illustrates how the feature vectors look. Consider the documents containing the single words, \textit{fail, sail}.

\vspace{10pt}
\begin{centering}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
	\hline 
	& f-a & f-i & f-l & a-i & a-l & i-l & s-a & s-i & s-l & a-t & l-i & l-t & i-t \\ 
	\hline 
	$\phi(fail)$ & $ \lambda^2 $ & $ \lambda^3 $ &$ \lambda^4 $  &  $ \lambda^2 $& $ \lambda^3 $  & $ \lambda^2 $ & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
	\hline 
	%$\phi(alit)$ & 0 & 0 & 0 & $\lambda^3$ & $\lambda^2$ & 0 & 0 & 0 & 0 & $\lambda^4$ & $\lambda^2$ & $\lambda^3$ & $\lambda^2$ \\ 
	%\hline 
	$\phi(sail)$ & 0 & 0 & 0 & $\lambda^2$ & $\lambda^3$ & $\lambda^2$ & $\lambda^2$ & $\lambda^3$ & $\lambda^4$ & 0 & 0 & 0 & 0 \\ 
	\hline 

\end{tabular} 
\captionof{table}{Feature vectors for two simple documents.}
\end{centering}

\vspace{10pt}
To calculate the similarity between the documents \textit{fail} and \textit{sail} we simply calculate the inner product between their respective feature vectors, resulting in the kernel value $ K_2(fail,sail)  = 2\lambda^4 + \lambda^6 $. An other example which we later used to prove that our implementation of the kernel was correct is the kernel values Lodhi et al. presents for the documents \textit{'science is organized knowledge'} and \textit{'wisdom is organized life'}. These were: $ K_1 = 0.580 $, $ K_2 = 0.580$, $ K_3 = 0.478, K_4=0.439, K_5 = 0.406, K_6 = 0.370 $.

To explicitly calculate all the features for documents present in the Reuters-21578 would require impractical amounts of computations, which is why Lohdi et al. presents a recursive formulation to calculate the kernel values. We will present them as they are in the paper, since we followed them exactly in our implementation. 

\subsubsection*{Definition - Recursive computation of the subsequence kernel}
\begin{align*}
	K'_{0}(s,t) & = 1, for\ all\ s,t \\
	K'_i(s,t) & = 0, if\ \min(|s|,|t|) < i \\
	K_i(s,t) & = 0 if\ \min(|s|,|t|) < i \\
	K'_i(sx,t) & = \lambda K'_i(s,t) + \sum_{j:t_j=x} K'_{i-1}(s,t[1:j-1])\lambda^{|t|-j+2}, \hspace{15pt} i = 1, \dots n-1 \\	
	K_{n}(sx,t) & = K_n(s,t) + \sum_{j:t_j = x}K'_n-1(s,t[1:j-1])\lambda^2
\end{align*}  
This recursive formulation leaves the computational complexity at $ \mathcal{O}(n|s||t|^2]) $ which can be improved using the efficient computation Lodhi et al. presents, through first evaluating

\begin{equation*}\label{key}
K''_i(sx,t) = \sum_{j:t_j = x}K'_{i-1}(s,t[1:j-1])\lambda^{|t|-j+2}.
\end{equation*}

Observing that $ K'_i(s,t) $ can be evaluated with the recursion:
\begin{equation*}\label{key}
K'_i(sx,t) = \lambda K'_i(s,t) + K''_i(sx,t).
\end{equation*}
One can then observe that $ K''_i(sx,tu) = \lambda^{|u|}K''_i(sx,t)$, in the case that x does not occur in u, and otherwise 
\begin{equation*}\label{key}
K''_i(sx,tx) = \lambda \left( K''_i(sx,t) + \lambda K'_{i-1}(s,t) \right).
\end{equation*} 
With this implementation the computational complexity reduces to $ \mathcal{O}(n|s||t|) $.

\subsection{n-gram kernel}
The n-gram kernel (NGK) transforms text documents to the feature space in a similar fashion as described above, but does instead only consider contiguous substrings of length n, \textit{n-grams}. The value of each dimension in the feature space of a document is thus the number of occurrences of that n-gram in the document. 

With this approach all information encoded in the word order in the document is lost. The method has nonetheless proven useful, as we will see later.

\subsection{Word kernel - om någon kan förklara detta bättre, plz do}
Lohdi et al. presents the word kernel (WK) used as a is a linear kernel measuring similarity between documents that are indexed by words using a variant of the \textit{tf-idf}, $ \log(1+tf) \cdot \log(n/df) $. Here tf is the term frequency and df is the document frequency while n is the number of documents. 


\subsection{Preprocessing of data}
Before 



\begin{itemize}
	\item Implemented naive kernel, recursive, approx, ngk, wk, (fastText?)
	
	\item Alignment to compare real and approx K
	
	\item Possible improvement implementation 
	
	\item Mention comibation of kernels and why we did not find it necessary to investigate it any further. 
	
\end{itemize}

