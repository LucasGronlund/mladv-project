%TODO
% - Intro to text classification (multilable)
\subsection*{Text classification}
In the area of machine learning, classification of unseen data is of great interest. For this to be possible feature vectors of the input data is needed which is trivial for some types of data, but not so much for others. Text documents is an example of one such kind. 

One method for feature extraction often used within the field of text classification is the \textit{kernel method}, transforming the data into high dimensional spaces and comparing similarity between data points through their inner product. Using the kernel trick, the explicit transform into this feature space can be avoided, which enables infinite dimensional features spaces. 

The article \textit{Text classification using string kernels} written by Lohdi et. al. presents a new method of performing this feature extraction which Lohdi et al names \textit{string subsequence kernel} (SSK). This proposed kernel is like a natural extension to the two other methods presented as a base line in the article, namely word kernel (WK) and n-gram kernel (NGK). How these kernels differ will be explained in following sections and then compared in the discussion. 

Kernels are however not the only successful type of method in the text classification field, neural networks (CNN's RCNN's ) as well as a method called \textit{fastText} have shown great results. \textbf{THESE MIGHT BE DISCUSSED LATER AS A POSSIBLE COMPETITOR TO THE SSK}.

% - Purpose of ssk report
% - Kernels
% - Explain wk and ngk
% - Other methods (not kernels)
