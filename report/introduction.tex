%TODO
% - Intro to text classification (multilable)
\subsection{Text classification}
In the area of machine learning, classification of unseen data is of great interest. For this to be possible feature vectors of the input data is needed which is trivial for some types of data, but not so much for others. Text documents is an example of one such kind. 

One method for feature extraction often used within the field of text classification is the \textit{kernel method}, transforming the data into high dimensional spaces and comparing similarity between data points through their inner product. Using the '\textit{kernel trick}' in the \textit{Support Vector Machine} (SVM) classifier the explicit transform into this feature space can be avoided making most computations manageable. 

The SVM is a binary classifier separating two classes of data with a maximum margin decision boundary. Since this is not of central interest in this or Lohdi et al.'s article, we suppose the details of this algorithm to be known.

The paper \textit{Text classification using string kernels} written by Lohdi et. al. presents a new method of performing this feature extraction which Lohdi et al. names \textit{String Subsequence Kernel} (SSK). This proposed kernel is a natural extension to the two other already existing methods presented as a base line in the article, namely \textit{Word Kernel} (WK) and \textit{N-Gram Kernel} (NGK). How these kernels differ will be explained in following sections and then compared in the discussion. 

Kernels are however not the only successful type of method in the text classification field, neural networks (CNN's RCNN's ) as well as a method called \textit{fastText} have shown great results. \textbf{THESE MIGHT BE DISCUSSED LATER AS A POSSIBLE COMPETITOR TO THE SSK}.

Throughout Lohdi et al. the dataset used for classification was the \textit{Reuters dataset} consisting of news articles from Reuters news agency, divided into 130 classes. These classes are however not mutually exclusive which is common for text data. A document can for example be tagged with both 'earn' and 'acquisition', which is an extension making the classification more involved and how this is handled will be described later. \textit{Modified Apte}, also known as ModeApte, was used to split this data set into 9603 training documents and 3299 test documents. \textbf{IF WRONG ERIK, PLZ CORRECT}. The documents were preprocessed through removing non-informative words (stop-words).

The aim of this paper is to replicate the results from Lohdi et al. regarding the SSK, where the emphasis is put to the approximative implementation of their kernel method. We have therefore used python packages as sklearn and numpy for supplying SVM algorithms and other conveniences. 

\textbf{We also propose and test a possible computational complexity reduction since this proved to the major downside using SSK.}
% - Purpose of ssk report
% - Kernels
% - Explain wk and ngk
% - Other methods (not kernels)
