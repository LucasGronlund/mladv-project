
In the area of machine learning, classification of unseen data is of great interest. For this to be possible feature vectors of the input data are needed which is trivial for some types of data, but not so for text documents. Transforming the data into a \textit{feature space} by quantifying some aspect of the document, and then compare similarity between data points through their inner product is called the \textit{kernel method}. Using the '\textit{kernel trick}' for the \textit{support vector machine} (SVM) classifier the explicit transform into this feature space can be avoided.% which in many cases is what enables the method to be used since the feature spaces can be of very high dimensionality, in some cases even infinite.

The paper \cite{lodhi} presents a new method of performing feature extraction from text documents which Lohdi et al. names \textit{string subsequence kernel} (SSK). The SSK is compared with two other feature extraction methods, namely \textit{word kernel} (WK) and \textit{n-gram kernel} (NGK). Kernels are not the only successful type of method in the text classification field, neural networks as well as a \textit{fastText} \cite{joulin2016bag} have shown great results. These are regarded as state of the art algorithms and will compared to the kernels methods presented above. 

Throughout \cite{lodhi} the dataset used for classification was the \textit{Reuters-21578 dataset} consisting of news articles from Reuters news agency. \textit{Modified Apte}, also known as ModeApte, was used to split this data set into 7769 training documents and 3019 test documents, divided into 90 classes. These classes are however not mutually exclusive which is common for text data, a document can for example be tagged with both 'earn' and 'acquisition' which is an extension making the classification more involved.

The aim of this report is to replicate the results in \cite{lodhi} regarding the SSK, where the emphasis is put on the approximative implementation. We have therefore used Python packages such as \texttt{Sklearn} and \texttt{Numpy} for supplying SVM algorithms and other conveniences, but written code for all three kernels ourselves. Further, we have put no effort into replicating the results regarding the combination of kernels Lohdi et al. presents since it did not improve the classification performance in any significant way. Finally, we also propose a possible computational complexity reduction of the SSK since this is the the major downside with the algorithm.

