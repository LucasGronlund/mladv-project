%TODO Consequences from test runs
\subsection{Performance on subset of Reuters dataset}
We can in table \ref{tab:appendix_skk_ngk} and \ref{tab:appendix_ssk_n} see the same pattern between our data and that presented in \cite{lodhi}; for higher values of the sequence length $ n $ we have a clear decrease in performance for both NGK and SSK. We had to, as mentioned in the results, limit the number of iterations performed because of computational time, which is why we in table \ref{tab:ngk_subset} and \ref{tab:ssk_subset} only have averages over the three best performing values of $ n $, 3, 4 and 5. What is worth mentioning here is that WK, in table \ref{tab:wk_subset}, performs consistently better than SSK, which only achieves tangential $ F_1 $-scores for two of the four classes. When then considering the computational time difference between the methods which with our implementation almost differed with two orders of magnitude in favor of WK, the practical limitation of SSK becomes apparent. 
%However, the general trend is still apparent, performance quickly decreases of for $ n > 6 $ for almost all classes in this subset of Reuters dataset. This was consistent across all configurations of the algorithm and the approximated version. 

\subsection{Approximating string subsequence kernel}
Our implementation of the aSSK proved to outperform the results presented in \cite{lodhi}, both for $ x=1000 $ and $ x=3000 $, see table \ref{tab:alignment}. This might be due to the fact that we tuned our SVM's weighting factor $ C $ for misclassified data. 

Our alignment scores shown in figure \ref{fig:alignment} also proves that already using just a few hundred vectors in $ \tilde{S} $ essentially finds the same kernel matrix using aSSK as when using SSK.

\subsection{Performance on entire Reuters dataset}
%Interestingly, the results for SSK isn't super promising, we usually achieve tangential performance with either NGK and WK. 
Using the entire Reuters dataset to train WK, NGK and aSSK we find in table \ref{tab:full_data} similar results as when only using the subset; WK performs better than both NGK and aSSK in a fraction of the time, even though WK is implemented in Python while aSSK is implemented in C++ which generally is a much faster programming language. 


%Varying $ \lambda $ shows that SSK is not that sensitive to values of $ \lambda $. Generally we can see that unless you choose more extreme values, the SSK performs well.


%This becomes painfully clear when we run the full dataset, where WK outperformed NGK and SSK on most categories at a fraction of the computation time, and that is with our approximation kernel exported to C++ and WK running completely in Python, which generally is a much slower programming language. 

%TODO
% - why wk is faster
% - klaga p√• repeatability (uppdelning av data, parametera osv)
% - 

As Lodhi et al. summarized the findings in \cite{lodhi}, small text strings and small $ n $ will the SSK perform well, but quickly loses performance. This can possibly be explained trough that when we have more data, noise to signal ratio for words decreases and we can rely more on the words rather than some sequence of characters. More documents also makes the  \textit{tfidf} transform better, since we get more information on what words contains information and which words are too frequent to offer distinguishing features. At the same time will NGK and the SSK encounter more noisy information, since more and longer texts are bound to hit more possible combinations. 

Another reason for why WK works well compared to the SSK might be because of the nature of the dataset. Reuters journalists are proofread and held to a higher linguistic standard than  a conversation from some messenger program. The relative lower amount of spelling errors and stylistic stringency should have a positive effect on WK's performance relative to the SSK. Important to note when comparing our results to \cite{lodhi} is that we use a slightly smaller split generated by ModeApte than what is presented in \cite{lodhi}. How Lodhi et al. handled their data is not always clearly stated, so we have had to make some assumptions, esecially regarding the preprocessing. It is further not clear if Lohdi et al. have used multi-label classification on the subset of the Reuters dataset and what values the SVM parameters had. This limits our ability to reproduce all results presented for higher values of $ n $ for the SSK, see table \ref{tab:appendix_ssk_n}.

As with the report from Lodhi et al, we find that SSK is not particularly sensitive to values on the decay parameter $ \lambda $, and as long as one avoids values close to one or zero the performance is generally very similar within classes.

Reuters, being an unbalanced dataset creates some issues for the SVM. The disparity between classes are big, sometimes thousands to one. We've tried to solve this problem with a variety of techniques, but mostly trying to balance the data by assigning relative weights between the classes. Time restrictions made tuning the SVM parameters to achieve acceptable results impossible for $ n \in [10,12,14] $. 


%TODO Similarity to fastText and CNN and differences

\subsection{Present state-of-the art algorithms}
Looking at the present state of natural language processing we find that currently quite a lot of hype surrounds algorithms like Convolutional Neural Networks (CNN) and \textit{fastText}. Neural networks can be notoriously slow to train, but generally performs very well across multiple classification and regression problems. \textit{fastText} on the other hand i very fast, capable of training and classifying large datasets in seconds \cite{joulin2016bag}. It uses language specific precomputed vectors to classify text, but has been shown to perform similarly to CNNs on some datasets. The main benefit is the drastically quicker computation time. To some extent it differs where you put the computational effort.

\subsection{Proposed improvement}
Our experiments on the first 100 documents in the Reuters dataset indicates that there are some decrease in computational time without losing much classification performance since the alignment approaches 1 while our proposed improvement still requires less time, see figure \ref{fig:improved_assk}. Since the Reuters data set contains plenty of documents with several thousand characters, introducing a cut-off might be more effective than for a dataset containing shorter documents. 

 This is of course highly dependent on the implementation of the algorithms, thus further investigations are needed for any conclusive results. 
