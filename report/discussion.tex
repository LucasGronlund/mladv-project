%TODO Consequences from test runs
\subsection{Performance on subset of Reuters dataset}
We can in table \ref{tab:appendix} see the same pattern between our data and that presetned in \cite{lodhi};  for higher values of the sequence length $ n $ we have a clear decrease in performance. We had to, as mentioned in the results, limit the number of iterations performed because of computational time, which is why we in table \ref{tab:ngk_subset} and \ref{tab:ssk_subset} only have averages over the three best performing values of $ n $. What is worth mentioning here is that WK, in table \ref{tab:wk_subset}, performs consistently better than SSK, which only achieves tangential $ F_1 $-scores for two of the four classes. When then considering the computational time difference between the methods which with our implementation almost differed with two orders of magnitude in favor of WK, the practical limitation of SSK becomes apparent. 
%However, the general trend is still apparent, performance quickly decreases of for $ n > 6 $ for almost all classes in this subset of Reuters dataset. This was consistent across all configurations of the algorithm and the approximated version. 

\subsection{Approximating string subsequence kernel}
Our implementation of the approximative SSK proved to outperform the results \cite{lodhi} presented, both for $ x=1000 $ and $ x=3000 $, see table \ref{tab:alignment}. This might be due to the fact that we tuned our SVM's weighting factor $ C $ for misclassified data. 

Our alignment scores shown in figure \ref{fig:alignment} also proves that already using just a few hundred vectors in $ \tilde{S} $ essentially finds the same kernel matrix using aSSK as when using SSK.

\subsection{Performance on entire Reuters dataset}
%Interestingly, the results for SSK isn't super promising, we usually achieve tangential performance with either NGK and WK. 
Using the entire Reuters dataset to train WK, NGK and aSSK we find in table \ref{tab:full_data} we find similar results as when only using the subset; WK performs better than both NGK and aSSK in a fraction of the time, even though WK is implemented in Python while aSSK is implemented in C++ which generally is a much faster language. 

%This becomes painfully clear when we run the full dataset, where WK outperformed NGK and SSK on most categories at a fraction of the computation time, and that is with our approximation kernel exported to C++ and WK running completely in Python, which generally is a much slower programming language. 

As Lodhi et al. summarized the findings in \cite{lodhi}, that for small text strings and small $ n $ will the SSK perform well, but quickly losses ground. This can be very logical, since when we get more data noise to signal ratio for words goes down and we can rely more on the words rather than some sequence of characters. More documents also makes the $ tfidf $ transform better, since we get more data on which words contains information and which words are too frequent to offer distinguishing features. At the same time will \textbf{stuff} like NGK and the SSK hit more noisy information since more and longer text string are bound to hit more possible combinations. Another reason for why WK works well compared to SSK might be because of the nature of the dataset. Reuters journalists are proofread and held to a higher linguistic standard than say a chat log from some messenger program. The relative lower amount of spelling errors and stylistic stringency should have a positive effect on WK's performance relative SSK. Important to note when comparing our results to \cite{lodhi} is that we use a slightly smaller subset of Reuters than presented there. How \cite{lodhi} handled their data is not always clearly stated, so some assumptions have been made. In some cases this might provide a higher $ F_1 $ score than \cite{lodh}.

As with the report from Lodhi et al, we find that SSK is not particularly sensitive to values on the decay parameter $ \lambda $, and as long as one avoids values close to one or zero the performance is generally very similar between classes.

Reuters, being a unbalanced dataset creates some issues for the SVM. The disparity between classes are big, sometimes thousands to one. We've tried to solve this problem with a variety of technique, but mostly trying to balance the data so that the SVM can find decision boundary that generalizes well. What we've found though is that we didn't manage to find appropriate parameters for larger $ n $. Both NGK and SSK suffered at higher values of $ n $ and we could not replicate the performance mentioned by Lodhi et al. with many classes performing zero precision and recall. Time restrictions made tuning the parameters for the SVM to achieve acceptable results impossible for $ n \in [10,12,14] $. 


%TODO Similarity to fastText and CNN and differences
Looking at the present state of natural language processing we find that currently quite a lot of hype surrounds algorithms like Convolutional Neural Networks (CNN) and \textit{fastText}. They work very differently, both from each other and SSK, but both perform very well. Use a deep network to classify text. Neural networks can be notoriously slow to train for large datasets, but generally performs very well across multiple classification and regression problems. \textit{fastText} on the other hand i very fast, capable of training and classifying large datasets in seconds SOURCE. It uses language specific precomputed vectors to classify text, but has been shown to perform similarly to CNNs on some datasets. The main benefit is the drastically quicker computation time.


We looked into a method of speeding up the computations further without losing to much accuracy. Our experiments on the same four category subset of Reuters as used by Lodhi et al showed that we can achieve some reduction in time.
