%TODO Consequences from test runs
We can clearly see the same pattern between our data and that of Lodhi et al, that for higher $ n $ we have a clear drop off in performance. We had to limit our computations because of time limitations, but the general trend is there. Performance quickly drops of for $ n>6 $ for almost all classes. This was consistent across all configurations of the algorithm and the approximated version. Interestingly, the results for SSK isn't super promising, we usually achieve tangential performance with either NGK and WK. This becomes painfully clear when we run the full dataset, where WK outperformed NGK and SSK on most categories at a fraction of the computation time, and that is with our approximation kernel exported to C++ and WK running completely in Python. As Lodhi summarized the findings in his report, that for small text strings and small $ n $ will the SSK perform well, but quickly losses ground. This can be very logical, since when we get more data noise to signal ratio for words goes down and we can rely more on the words rather than some sequence of characters. More documents also makes the $ tfidf $ transform better SOURCE. At the same time will stuff like n-grams and the SSK hit more noisy information since more and longer text string are bound to hit more possible combinations. Another reason for why WK works well compared to SSK might be because of the nature of the dataset. Reuters journalists are proofread and held to a higher linguistic standard than say a chat log from some messenger program. The relative lower amount of spelling errors and stylistic stringency should have a positive effect on WK's performance relative SSK. Important to note when comparing our results to \ref{lodhi2002text} is that we use a slightly smaller subset of Reuters than them. How they handled their data is not always clearly stated, so some assumptions have been made. In some cases this might provide a higher $ F_1 $ score than \ref{lodhi2002text}.
\\
As with the report from Lodhi et al, we find that SSK is not particularly sensitive to values on the decay parameter $ \lambda $, and as long as one avoids values close to one or zero the performance is generally very similar between classes. Reuters, being a unbalanced dataset creates some issues for the SVM. The disparity between classes are big, sometimes thousands to one. We've tried to solve this problem with a variety of technique, but mostly trying to balance the data so that the SVM can find decision boundary that generalizes well. What we've found though is that we didn't manage to find appropriate parameters for larger $ n $. Both NGK and SSK suffered at higher values and we could not replicate the performance mentioned by Lodhi et al.  
\\
%TODO Similarity to fastText and CNN and differences
Looking at the present state of natural language processing we find that currently quite a lot of hype surrounds algorithms like Convolutional Neural Networks (CNN) and \textit{fastText}. They work very differently, both from each other and SSK, but both perform very well. Use a deep network to classify text. Neural networks can be notoriously slow to train for large datasets, but generally performs very well across multiple classification and regression problems. \textit{fastText} on the other hand i very fast, capable of training and classifying large datasets in seconds SOURCE. It uses language specific precomputed vectors to classify text, but has been shown to perform similarly to CNNs on some datasets. The main benefit is the drastically quicker computation time.
\\
We looked into a method of speeding up the computations further without losing to much accuracy. Our experiments on the same four category subset of Reuters as used by Lodhi et al showed that we can achieve some reduction in time.
