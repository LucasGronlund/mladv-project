%TODO Consequences from test runs
We can clearly see the same pattern between our data and that of Lodhi et al, that for higher $ n $ we have a clear drop off in performance. We had to limit our computations because of time limitations, but the general trend is there. Performance quickly drops of for $ n>6 $ for almost all classes. This was consistent across all configurations of the algorithm and the approximated version. Interestingly, the results for SSK isn't super promising, we usually achieve tangential performance with either NGK and WK. This becomes painfully clear when we run the full dataset, where WK outperformed NGK and SSK on most categories at a fraction of the computation time, and that is with our approximation kernel exported to C++ and WK running completely in Python. As Lodhi summarized the findings in his report, that for small text strings and small $ n $ will the SSK perform well, but quickly losses ground. This can be very logical, since when we get more data noise to signal ratio for words goes down and we can rely more on the words rather than some sequence of characters. More documents also makes the $ tfidf $ transform better SOURCE. At the same time will stuff like n-grams and the SSK hit more noisy information since more and longer text string are bound to hit more possible combinations. Another reason for why WK works well compared to SSK might be because of the nature of the dataset. Reuters journalists are proofread and held to a higher linguistic standard than say a chat log from some messenger program. The relative lower amount of spelling errors and stylistic stringency should have a positive effect on WK's performance relative SSK.
\\
As with Lodhi et al's report, we find that SSK is not particularly sensitive to values on the decay parameter $ \lambda $, and as long as one avoids values close to one or zero the performance is generally very similar between classes. What however is very sensitive is the SVM for the unbalanced data that Reuters have. The disparity between classes are big, sometimes thousands to one. We've tried to solve this problem with a variety of technique, but mostly trying to balance the data so that the SVM can find decision boundary that generalizes well. What we've found though is that we didn't manage to find appropriate parameters for larger $ n $. Both NGK and SSK suffered at higher values and we could not replicate the performance mentioned by Lodhi et al.  

%TODO Similarity to fastText and CNN and differences
Looking at how the String Subsequence Kernel behaves in comparison to for example Convoluted Neural Networks and Facebook's fastText algorithm. These both perform very well in different ways. CNN's has been shown to be able to reach very high level of performance, but takes long time to train and can be difficult to configure. fastText is very fast, training times in seconds rather than hours or days, but requires language specific understanding to work. fastText has been shown to be similiarly accurate to CNNs, but at a fraction of the computation time. fastText uses a linear classifier in its base, but uses very large, precomputed, feature vectors and several computational tricks to speed up training significantly.